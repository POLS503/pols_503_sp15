% Copyright (C) 2015 Jeffrey B. Arnold
% License CC BY-NC-SA 4.0 http://creativecommons.org/licenses/by-nc-sa/4.0/
<<header,echo=FALSE,results='hide',warning=FALSE,message=FALSE>>=
source("init.R")
@

\input{\jobname-options}
\ifdefined\ishandout
  \documentclass[handout]{beamer}
\else
  \documentclass[]{beamer}
\fi

% HEADER HERE

\input{includes.tex}


\newcommand{\thetitle}{Matrix Algebra, Linear Regression}
\date{April 7, 2015}
\title{\thetitle{}}
\hypersetup{
  pdftitle={\thetitle{}},
  pdfkeywords={statistics}
}
\begin{document}

\begin{frame}
  \maketitle{}
\end{frame}

\begin{frame}
  \frametitle{Agenda}
  
  \begin{itemize}
  \item Linear regression as finding a ``best'' line
  \item Linear regression as the conditional expectation function
  \item How linear regression relates to the normal distribution
  \end{itemize}
  
\end{frame}

\begin{frame}
  \frametitle{What is regression?}
  
  \begin{block}{Regression}
    distribution of a \textbf{response} (outcome) variable $Y$ --- or summary of that distribution --- 
    as a function of \textbf{explanatory} variables $X_{1}, \dots, X_{k}$.
  \end{block}
  
  \begin{block}{Ordinary Least Squares}
    Finds a $\hat{Y} = \mat{X} \vc{B}$ that minimizes $\sum (Y_{i} - \hat{Y}_{i})^{2})$.
    This estimates a linear conditional expectation function $E(Y | X_{1}, \dots, X_{k})$.
  \end{block}
  
\end{frame}

\begin{frame}
  \frametitle{OLS Objective Function}
  \framesubtitle{One $X$}
  
  Find the line 
  \begin{equation*}
    \hat{Y} = A + B X
  \end{equation*}
  such that
  \begin{equation*}
    A, B = \argmin_{A, B} S(A, B)
  \end{equation*}
  where 
  \begin{equation*}
    S(A, B) = \sum_{i} E_{i}^{2} = \sum_{i} (Y_{i} - \hat{Y}_{i})^{2} = \sum_{i} (Y_{i} - A - B X_{i})^{2}
  \end{equation*}
  
  How do we minimize this? 
    
\end{frame}

\begin{frame}
\frametitle{What does the OLS objective function look like?}

<<fakeregdata>>=
n <- 100
x <- runif(n, -1, 1)
err <- rnorm(n, 0, 0.7)
y <- 1 + 2 * x + err
dat <- data_frame(x = x, y = y, err = err)

(ggplot(dat, aes(x = x, y = y))
 + geom_point() 
 + theme_minimal()
 + geom_abline(intercept = 1, slope = 2, colour = "red")
 + geom_abline(intercept = 0, slope = 0, colour = "cyan")
)

@

Data generated by $Y_i = 1 + 2 X_i + E_i$. Lines are $A = 1, B = 2$, and $A = 0, B = 0$. 

\end{frame}

\begin{frame}
  \frametitle{$\sum E_{i}^{2}$ as a function of $A$ and $B$}
  \framesubtitle{Least squares is the minimum of this function}

<<contourplot>>=

f <- Vectorize(function(a, b) {
  sum((dat$y - a - b * dat$x)^2)
})

dat2 <- expand.grid(A = seq(-3, 5, by = 0.05),
                    B = seq(-3, 5, by = 0.05)) %>%
  mutate(e2 = f(A, B))

(ggplot(dat2, aes(x = A, y = B, z = e2))
 + geom_raster(aes(fill = e2))
 + stat_contour()
 + scale_x_continuous(expand = c(0,0))
 + scale_y_continuous(expand = c(0,0)) 
 + annotate("point", x = 1, y = 2, colour = "red")
 + annotate("point", x = 0, y = 0, colour = "cyan")
 + theme_minimal()
)

@
\end{frame}

\begin{frame}
  \frametitle{Finding the best $A, B$ in Least Squares}
  \framesubtitle{One $X$}
  
  To minimize, set partial derivatives equal to 0 and solve:
  \begin{align*}
    \frac{\partial S(A, B)}{\partial A} &= \sum (-1) (2) (Y_{i} - A - B X_{i}) = 0 \\
    \frac{\partial S(A, B)}{\partial B} &= \sum (-X_{i}) (2) (Y_{i} - A - B X_{i}) = 0
  \end{align*}
  Rearrange to get
  \begin{align*}
    A &= \bar{Y} - B \bar{X} \\
    B &= \frac{\sum (X - \bar{X}) (Y - \bar{Y})}{ \sum (X_{i} - \bar{X})^{2}} = \frac{\cov(X, Y)}{\var(X)}
  \end{align*}
  
\end{frame}

\begin{frame}
  \frametitle{Implications of the OLS Solution}
  \small
  
  Least squares $A$ and $B$
  \begin{align*}
    A &= \bar{Y} - B \bar{X} \\
    B &= \frac{\sum (X - \bar{X}) (Y - \bar{Y})}{ \sum (X_{i} - \bar{X})^{2}} = \frac{\cov(X, Y)}{\var(X)}
  \end{align*}
  
  \begin{itemize}
  \item $\bar{X}, \bar{Y}$ is in the regression line
  \item $\sum X_{i} E_{i} = 0$
    \begin{align*}
      \sum X_{i} E_{i} &= \sum X_{i} (Y_{i} - A - B X_{i})  \\
      &= \sum X_{i} Y_{i} - A \sum X_{i} - B \sum X_{i} = 0
    \end{align*}
  \item $\sum \hat{Y}_{i} E_{i} = 0$
  \item Errors $E$ uncorrelated with $\hat{Y}$ and $X$
    %% \begin{align*}
    %%   \sum \hat{Y}_{i} E_{i} &= \sum (A - B X_{i}) (Y_{i} - A - B X_{i})  \\
    %% \end{align*}
  \end{itemize}
  
\end{frame}


\begin{frame}
  \frametitle{OLS Objective Function}
  \framesubtitle{Multiple $X$}
  
  Find plane
  \begin{equation*}
    Y = A + B_{1} X_{1} + B_{2} X_{2} + \dots + B_{k} X_{k}
  \end{equation*}
  such that 
  \begin{equation*}
    A, B_{1}, \dots, B_{k} = \argmin_{A, B_{1}, \dots, B_{k}} S(A, B_{1}, \dots, B_{k})
  \end{equation*}
  where
  \begin{align*}
    S(A, B_{1}, \dots, B_{k}) &= \sum_{i} E_{i}^{2} = \sum_{i} (Y_{i} - \hat{Y}_{i})^{2}  \\
            &= \sum_{i} (Y_{i} - A - \sum_{j=1}^{k} B_{j} X_{i,j})
  \end{align*}
  
  How do we minimize this?
\end{frame}


\begin{frame}
  \frametitle{Finding the best $A, B$ in Least Squares Regression}
  \framesubtitle{Multiple $X$}
  
  Set partial derivatives equal to 0 and solve system of equations for 
  {\footnotesize
  \begin{align*}
    \frac{\partial S(A, B_{1}, B_{2}, \dots, B_{k})}{\partial A} &= \sum (-1) (2) (Y_{i} - A - B X_{i}) = 0 \\
    \frac{\partial S(A, B_{1}, B_{2}, \dots, B_{k})}{\partial B_{1}} &= \sum (-X_{i,1}) (2) (Y_{i} - A - B_{1} X_{i,1} - \dots - B_{2} X_{i,k}) = 0 \\
    \vdots &= \vdots \\
    \frac{\partial S(A,  B_{1}, B_{2}, \dots, B_{k})}{\partial B_{k}} &= \sum (-X_{i, k}) (2) (Y_{i} - A - B_{1} X_{i,1} - \dots - B_{2} X_{i,k}) = 0 \\
  \end{align*}
  }

  \only<2>{Not as easy \dots}
  
\end{frame}


\begin{frame}
  \frametitle{Linear Regression in Matrix Form}
  
  Scalar representation
  \begin{equation*}
    Y_{i} = B_{0} + B_{1} X_{i,1} + B_{2} X_{i,2} + \dots B_{k} X_{i,k} + E_{i}
  \end{equation*}
  Equivalent matrix representation
  \begin{equation*}
    \underset{n \times 1}{\mat{y}} = \underset{n \times (k + 1)}{\mat{X}} \quad \underset{(k + 1) \times 1}{\vc{b}} + \underset{n \times 1}{\vc{e}}
  \end{equation*}
  or 
  \begin{equation*}
    \begin{bmatrix}
      Y_{1} \\
      Y_{2} \\
      \vdots \\
      Y_{n}
    \end{bmatrix}
    = 
    \begin{bmatrix}
      1 & X_{1,1} & X_{2,1} & \cdots & X_{k,1} \\
      1 & X_{1,2} & X_{2,2} & \cdots & X_{k,2} \\
      \vdots & \vdots & \vdots & \ddots & \vdots \\
      1 & X_{1,n} & X_{2,n} & \cdots & X_{k,n} \\
    \end{bmatrix}
    \begin{bmatrix}
      B_{0} \\
      B_{1} \\
      \vdots \\
      B_k
    \end{bmatrix}
    + 
    \begin{bmatrix}
      E_{1} \\
      E_{2} \\
      \vdots \\
      E_{n}
    \end{bmatrix}
  \end{equation*}
  
  % Adolph
\end{frame}

\begin{frame}
  \frametitle{Linear Regression in Matrix Form}
  \framesubtitle{Objective Function}
  
  The linear regression is 
  \begin{equation*}
    \vc{y} = \mat{X} \vc{b} + \vc{e}
  \end{equation*}

  Want to find the $b$ that minimizes the squared errors:
  \begin{align*}
    \argmin_{\vc{b}} S(\vc{b})
  \end{align*}
  where
  \begin{align*}
    S(\vc{b}) &=  \sum E_{i}^{2} = \vc{e}' \vc{e}  \\
              &= (\vc{y} - \mat{X} \vc{b})' (\vc{y} - \mat{X} \vc{b})
  \end{align*}
  
  Why does $\vc{e}$ need to be transposed? 

\end{frame}

\begin{frame}
  \frametitle{Linear Regression in Matrix Form}
  \framesubtitle{Transpose of Sums}
  
    \begin{align*}
      (A + B)' &= A' + B' \\
      \left(
      \begin{bmatrix}
        10 \\ 
        3
      \end{bmatrix}
      + 
      \begin{bmatrix}
        2 \\
        6
      \end{bmatrix}
      \right) '
      &= ? \\
      ? &= ? 
    \end{align*}
  
\end{frame}

\begin{frame}
  \frametitle{Linear Regression in Matrix Form}
  \framesubtitle{Transpose of a product}
  
  \begin{align*}
    \left(
    \mat{X} \mat{B} 
    \right)'
    &= \mat{B}' \mat{X}' \\
    \begin{bmatrix}
      2 & 1 \\
      5 & 6 \\
    \end{bmatrix}
    \begin{bmatrix}
      3 \\
      4 \\
    \end{bmatrix}
    &= ? \\
    ? &= ?
  \end{align*}

\end{frame}

\begin{frame}
  \frametitle{Simplify $\vc{e}'\vc{c}$}
  
  \footnotesize
  \begin{align*}
    \vc{e}' \vc{e} &= (\vc{y} - \mat{X} \vc{b})' (\vc{y} - \mat{X} \vc{b}) \\
                   &= (\vc{y}' - (\mat{X} \vc{b})') (\vc{y} - \mat{X} \vc{b}) && \text{distribute the transpose} \\
    &= (\vc{y} -  \vc{b}' \mat{X}) (\vc{y} - \mat{X} \vc{b}) && \text{substitute $\vc{b}' \mat{X}'$ for $(\mat{X} \vc{b})'$} \\
    &= \vc{y}' \vc{y} - \vc{b}' \mat{X}' \vc{y} - \vc{y}' \mat{X} \vc{b} + \vc{b}' \mat{X}' \mat{X} \vc{b} && \text{multiply out} \\
    &= \vc{y}' \vc{y} - 2 \vc{b}' \mat{X}' \vc{y} + \vc{b}' \mat{X}' \mat{X} \vc{b} && \text{simplify}
  \end{align*}
  
  \begin{itemize}
  \item To minimize need to calculate derivative of $\vc{e}' \vc{e}$ with respect to $\vc{b}$.
  \item Need two know two things
    \begin{itemize}
    \item derivative of scalar with respect to vector ($2 \vc{b}' X' y$)
    \item derivative of quadratic form ($\vc{b}' \mat{X}' \mat{X} \vc{b}$)
    \end{itemize}
  \end{itemize}
  
\end{frame}

\begin{frame}
  \frametitle{What is the derivative of scalar with respect to vector}
  
  \begin{itemize}
  \item Need to take derivative of $\vc{e}' \vc{e}$ with respect to $\vc{b}$ to find $\vc{b}$ that min the sum of squared.
  \item A derivative of a scalar with respect to a vector
  \begin{align*}
    y = \vc{a}' \vc{x} &= a_{1} x_{1} + a_{2} x_{2} + \dots + a_{n} x_{n}\\
    \frac{\partial y}{\partial \vc{x}} &=
                                         \begin{bmatrix}
                                           a_{1} & a_{2} & \dots & a_{n}
                                         \end{bmatrix}' \\
    \frac{\partial y}{\partial \vc{x}} &= \vc{a}
  \end{align*}
  \end{itemize}
  
  
\end{frame}

\begin{frame}
  \frametitle{Derivative of a quadratic form}
  
  \begin{itemize}
  \item Equivalent to $x^{2}$ is inner product $\vc{x}' \vc{x}$
  \item Vector analogue of $a x^{2}$ is $\vc{x}' \mat{X} \vc{x}$, where $A$ is $n \times n$ matrix
    \begin{align*}
      \frac{\partial a x^{2}}{\partial x} &= 2 a x \\
      \frac{\partial \vc{x}' A \vc{x}}{\partial \vc{x}} &= 2 \mat{A} \vc{x}
    \end{align*}
  \end{itemize}
  
\end{frame}

\begin{frame}
  \frametitle{OLS in Matrix Form}
  \framesubtitle{Minimizing the objective function}
  
  \begin{enumerate}
  \item Take partial derivative of $S(\vc{b})$:
    \begin{align*}
      \frac{\partial S(\vc{b})}{\vc{b}}  &= \frac{\partial}{\vc{b}} (\vc{y}' \vc{y} - 2 \vc{b}' \mat{X}' \vc{y} + \vc{b}' \mat{X}' \mat{X} \vc{b}) \\
      &= 0 - (2 \vc{y}' \mat{X} ) + 2 (\mat{X}' \mat{X}) \vc{b}
    \end{align*}
  \item Set to 0, and solve for $\vc{b}$:
    \begin{align*}
      \mat{X}' \mat{X} \vc{b} &= \mat{X}' \vc{y} \\
      \vc{b} &= (\mat{X}' \mat{X})^{-1} \mat{X}' \vc{y}
    \end{align*}
  \end{enumerate}
  
\end{frame}

\begin{frame}
  \frametitle{What $(X' X)^{-1}$ implies}
  
  \begin{itemize}
  \item For $\vc{b}$ to be defined $(X' X)^{-1}$ needs to exist
  \item $X' X$ must be full rank
  \item rank of $X' X$ is the same as the rank of $X$
    
  \item The rank of $X$ is between $n$ and $k + 1$, means that $n \geq k + 1$ (obs > variables)
  \item $k + 1$ columns of $X$ must be linearly indepdendent? 
    \begin{itemize}
    \item Can you have a full set of dummies? 
    \item Can you include a variable that is always equal to 3? 
    \end{itemize}
  \end{itemize}
  
\end{frame}

\begin{frame}
  \frametitle{Takeaways}
  
  \begin{itemize}
  \item Linear regression is the $A, B_{1}, \dots, B_{k}$ that solve $\argmin_{A, B_{1}, \dots, B_{k}} \sum E_{i}^{2}$
  \item Solving for linear regression coefficients is relatively \textbf{easy}; linear equations; there's an explicit solution. No iteration required.
  \end{itemize}
\end{frame}

\section{Linear Regression and CEF}

\begin{frame}
  \frametitle{CEF justification for linear regression justification}
  
  \begin{itemize}
  \item Conditional Expectation Function is $\E(Y_{i} | X_{i} =x)$ for all $x$
  \item The CEF is the Min Mean Squared Error (MMSE) predictor of $Y_{i}$ given $X_{i}$
  \item If the population CEF is linear, then the least squares population regression is the CEF
  \item If the population CEF is not linear, then the least squares line is the MMSE linear estimate of the CEF.
    
  \item See Angrist and Pischke, Ch 3.1
  \end{itemize}
  
\end{frame}

\section{Linear Regression and Normal Distribution}

\begin{frame}
  \frametitle{But I thought linear regression had to do with the normal distribution?}

  \begin{itemize}
  
  \item Linear regression often presented as 
    \begin{align*}
      y_{i} &= X_{i} \beta + \epsilon_{i} && \epsilon_{i} \sim N(0, \sigma^{2})
    \end{align*}
  \item Why? We haven't had to assume normal distributions before now.
  \item Helps with statistical inference results. 
  \item However, the CLT handles asymptotic sampling distribution of parameters
  \end{itemize}
\end{frame}

%% \begin{frame}
%%   \item The normal distribution is 
%%     \begin{align*}
%%       f(x | \mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \exp 
%%       \left(
%%       - \frac{(x - \mu)^{2}}{2 \sigma^{2}}
%%       \right)
%%     \end{align*}
%%   \item To find 
%%     \begin{align*}
%%       \argmax_{\mu} \log f(x | \mu, \sigma) = \argmax_{\mu} \frac{(x - \mu)^{2}}{2 \sigma^{2}}
%%     \end{align*}
%%   \item How does that relate to 
%%   \item Normal errors to linear regression like normal distribution to the mean. 
%%   \end{itemize}
  
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Random Variables}
%%   We can think of social science variables as comprised of two parts:  
  
%%   \begin{description}
%%   \item[Systemic] Determined by social relationships
%%     \begin{itemize}
%%     \item e.g. income determined by education
%%     \end{itemize}
%%   \item[Stochastic] Naturally occuring random variation
%%     \begin{itemize}
%%     \item e.g. not everyone with same characterstics has same income
%%     \end{itemize}
%%   \end{description}
  
%%   \begin{itemize}
%%   \item \textbf{Random variables} contain both components
%%   \item Random variables are understood using probability distributions
%%   \end{itemize}

%%   % Adolph
%% \end{frame}


%% \begin{frame}
%%   \frametitle{Probability distributions}

%%   \begin{block}{Probability distribution}
%%     A probability distribution describes in a random variable precisely.
%%     Suppose Y is a random variable. We can summarize it in two ways:
%%     \begin{description}
%%     \item[pdf] probability density function, $f (Y)$. For any possible value of $Y = y$, gives the probability that $Y$ takes on that value.
%%     \item[cdf] cumulative density function, $F (Y)$
%%       For any possible value of $Y = y$, gives the probability that Y is less than or equal to that value.
%%     \end{description}
%%   \end{block}

%% % Adolph
%% \end{frame}


%% \begin{frame}
%%   \frametitle{Expected Value}
  
%%   Population ``mean''
  
%%   \begin{description}
%%     \item[discrete] distribution
%%       \begin{equation*}
%%         E(Y) = \sum_{i} p(Y_{i}) Y_{i}
%%       \end{equation*}
%%     \item[continuous] distribution
%%       \begin{equation*}
%%         E(Y_{i}) = \int_{i} p(Y) Y dY
%%       \end{equation*}
%%     \end{description}
%% \end{frame}

%% \begin{frame}
  
%%   \begin{block}{Questions}
%%     What are the expected values of:
%%     \begin{itemize}
%%     \item Discrete distribution 
%%       \begin{equation*}
%%       \begin{array}{l|lll}
%%         Y & -1 & 0 & 1 \\
%%         p(Y) & 0.2 & 0.2 & 0.7
%%       \end{array}
%%       \end{equation*}
%%     \item Normal distribution $N(-2, 3^{2})$
%%     \end{itemize}
%%   \end{block}
  
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Conditional Expectation Function}
  
%%   \begin{align*}
%%     E(Y | X) &= \sum_{i} p(X_{i}) p(Y | X_{i}) & \text{if $X$ discrete} \\
%%              &= \int p(X) p(Y | X) & \text{if $X$ continuous}
%%   \end{align*}
  
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Linear Regression and the Conditional Expectation Function}
%% \end{frame}

\section{Interpretation}

\begin{frame}
  \frametitle{Interpreting Regression Coefficients $\beta$}

  How the average outcome variable differs, on average: 
  \begin{description}
  \item[predictive] between \textbf{groups of units} that differ by 1 in the relevant explanatory variable while being identical in all other explanatory variables the same
  \item[counterfactual] in the \textbf{same individual} when chaning the relevant explanatory variable 1 unit while holding all other explanatory variables the same
  \end{description}
  
  See Gelman and Hill, p. 34; Fox, p. 81
\end{frame}


\begin{frame}
  \frametitle{References}
  
  \begin{itemize}
  \item Some slides derived from Christopher Adolph \textit{Linear Regression in Matrix Form / Propoerties \& Assumptions of Linear Regression}. Used with permission.
  \item Material included from
    \begin{itemize}
    \item Fox Ch 2, 5, 9.1--9.2
    \item Angrist and Pischke, Chapter 3.1
    \item Gelman and Hil, Chapter 2
    \end{itemize}
  \end{itemize}
  
\end{frame}

\end{document}
%  LocalWords:  CSSS MLE NHST r4stats KDnuggets Kagglers' TIOBE useR
%  LocalWords:  RedMonk Wickham
