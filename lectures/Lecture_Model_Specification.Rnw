% Copyright (C) 2015 Jeffrey B. Arnold
% License CC BY-NC-SA 4.0 http://creativecommons.org/licenses/by-nc-sa/4.0/

<<init,echo=FALSE,results='hide',warning=FALSE,message=FALSE>>=
LECTURE_NUM <- "Transformation"
source("init.R")
@
<<header>>=
suppressPackageStartupMessages({
  library("gapminder")
  library("assertthat")
  library("pols503")
})
@

\input{\jobname-options}
\ifdefined\ishandout%
  \documentclass[handout]{beamer}
\else
  \documentclass[]{beamer}
\fi

% HEADER HERE
\input{includes.tex}

\usepackage{verbatim}

\newcommand{\thetitle}{Model Specification and Fit}
\date{May 12, 2015}
\title{\thetitle{}}
\hypersetup{
  pdftitle={\thetitle{}},
  pdfkeywords={statistics}
}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\begin{frame}
  \maketitle{}
\end{frame}

\begin{frame}
\frametitle{Overview}
\tableofcontents
\end{frame}

\begin{frame}
  \frametitle{How To Choose Among Different Models?}

  \begin{itemize}
  \item Depends on your purpose
  \item Some tools
    \begin{itemize}
    \item Internal model validation: residuals, outliers
    \item Overall model Fit statistics: out of sample is preferred
    \end{itemize}
  \end{itemize}

\end{frame}

\section{Measures of Fit}

\begin{frame}
  \frametitle{Measures of Model Fit}

  Various measure of how the model fits the data, both \textit{in-sample} and \textit{out-of-sample}

\end{frame}

\subsection{$R^{2}$}

\begin{frame}
  \frametitle{The Coefficient of Determination, $R^{2}$}

  \begin{equation*}
    \begin{aligned}[t]
      R^{2} &= \frac{\text{Explained sum of squares}}{\text{Total sum of squares}}
      = 1 - \frac{\text{Residual sum of squares}}{\text{Total sum of squares}} \\
      &= \frac{\sum (\hat{y} - \bar{y})^{2} }{\sum (\hat{y} - \bar{y})^{2}} \\
      &= 1 - \frac{\sum \hat{\epsilon}^{2}}{\sum (\hat{y} - \bar{y})^{2}}
    \end{aligned}
  \end{equation*}

  \begin{itemize}
  \item Commonly used
  \item Ranges between
  \item Why can it never be less than 0?
  \item What happens when you add a variable?
  \item What is the case when $R^{2} = 1$
  \item Bivariate case: $\cor(y, x)^{2}$
  \item General case: $\cor(y, \hat{y})^{2}$
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{What $R^2$ does and doesn't say}

  \begin{itemize}
  \item Indirectly reports scatter around the regression line
  \item Only \textit{in sample}
  \item Maximizing $R^{2}$ perverse:
    \begin{itemize}
    \item Not usually interesting for explanation. $Y$ regressed on itself, vote choice on vote intention.
    \item Not usually best for prediction
    \end{itemize}
  \item Not an estimate
  \end{itemize}
\end{frame}


\begin{frame}
\frametitle{$R^2$ varies between samples}

<<>>=
sim_R2 <- function(iter, n, mu_X, s_X, R_X, beta, r_squared) {
  # Error checking so that bugs are caught quicker :-)
  assert_that(length(s_X) == length(mu_X),
              ncol(R_X) == nrow(R_X),
              ncol(R_X) == length(mu_X),
              length(beta) == (length(mu_X) + 1))
  # Generate an X
  X <- MASS::mvrnorm(n, mu = mu_X, Sigma = sdcor2cov(s_X, R_X),
                     empirical = TRUE)
  sigma <- r2_to_sigma(cbind(1, X), beta, r_squared)
  # Create a list to stor the results
  iterations <- list()
  # Loop over the simulation runs
  for (j in 1:iter) {
    # Draw y
    mu <- cbind(1, X) %*% beta
    epsilon <- rnorm(n, mean = 0, sd = sigma)
    y <- mu + epsilon
    # Run a regression
    mod <- lm(y ~ X)
    # Save the coefficients in a data frame
    mod_df <- glance(mod)
    iterations[[j]] <- mod_df
  }
  # Combine the list of data frames into a single data frame
  bind_rows(iterations)
}

r2s <- data_frame(n = c(128, 1024), r2 = 0.5) %>%
  group_by(n, r2) %>%
  do(sim_R2(1024, .$n, 0, 1, matrix(1), c(0, 1), .$r2))

ggplot(r2s, aes(x = r.squared, fill = factor(n))) +
  geom_density(alpha = 0.2) +
  scale_fill_discrete("sample size") +
  theme_minimal()

@

$R^{2}$ of samples drawn from a linear model with a population $R^2 = 0.5$.

\end{frame}

\begin{frame}
\frametitle{$R^2$ is a function of variation in $X$}

<<>>=
sample_by_r2 <- function(n, mu_X, s_X, R_X, beta, r_squared) {
  # Error checking so that bugs are caught quicker :-)
  assert_that(length(s_X) == length(mu_X),
              ncol(R_X) == nrow(R_X),
              ncol(R_X) == length(mu_X),
              length(beta) == (length(mu_X) + 1))
  # Generate an X
  X <- MASS::mvrnorm(n, mu = mu_X, Sigma = sdcor2cov(s_X, R_X),
                     empirical = TRUE)
  sigma <- r2_to_sigma(cbind(1, X), beta, r_squared)
  mu <- cbind(1, X) %*% beta
  epsilon <- rnorm(n, mean = 0, sd = sigma)
  y <- mu + epsilon
  # Loop over the simulation runs
  .data <- as.data.frame(X)
  .data$y <- as.numeric(y)
  .data
}

smpl1 <- sample_by_r2(1024, 0, 1, matrix(1), c(0, 1), 0.7) %>%
  mutate(within = V1 > -0.75 & V1 < 0.75)

stat1 <- glance(lm(y ~ V1, data = smpl1))
stat2 <- glance(lm(y ~ V1, data = filter(smpl1, within)))

ggplot(smpl1, aes(x = V1, y = y, colour = within)) +
  geom_point() + theme_minimal() +
  guides(colour = FALSE) + xlab("x")

@

\begin{itemize}
\item Complete sample (red + blue): $R^2 = \Sexpr{round(stat1$r.squared, 2)}$, $\hat\sigma = \Sexpr{round(stat1$sigma, 2)}$
\item Restricted sample (blue only): $R^2 = \Sexpr{round(stat2$r.squared, 2)}$, $\hat\sigma = \Sexpr{round(stat2$sigma, 2)}$

\end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Adjusted $R^{2}$}
  \framesubtitle{What's adjusted?}

  \begin{equation*}
    \begin{aligned}[t]
      \tilde{R}^{2} &= 1 - \frac{S_{E}^{2}}{S^{2}_{Y}} \\
      &= 1 - \frac{n - 1}{n - k - 1} \times \frac{RSS}{TSS}
    \end{aligned}
  \end{equation*}

  \begin{itemize}
  \item Where $n$ is number of obs, $k$ is number of variables.
  \item Unlike $R^{2}$, treat squared error terms as estimates of populatio, not sample statistics.
  \item How adjusted $R^{2}$ change with respect to $n$? With respect to $k$?
  \item But it is an ad hoc adjustment
  \end{itemize}

\end{frame}

\subsection{Standard Error of the Regression}

\begin{frame}
  \frametitle{Standard Error of the Regression}
  
  The standard error of the regression is the estimate of the population $\sigma$:
  \begin{equation*}
    \hat{\sigma}_{\epsilon} = S_{E} = \sqrt{\frac{\sum E_{i}^{2}}{n - k - 1}} = \sqrt{\frac{\sum_{i} (y_{i} - \hat{y}_{i})^{2}}{n - k - 1}}
  \end{equation*}
  
  \begin{itemize}
  \item $S_E$ is at least as useful to report as $R^{2}$
  \item $S_{E}$: on average, how much does the fitted value miss the actual value.
  \item On the same scale as $y$. Easier for interpretation and substantive importance.
  \end{itemize}
  
\end{frame}

\subsection{Information Criteria}

\begin{frame}
  \frametitle{Likelihood Function}
  
  \begin{itemize}
  \item Likelihood is the probability of observing the data given a statistical model.
  \item The \textbf{likelihood} of a linear model with normal errors:
    \begin{align*}
      L(\hat{\beta}, \hat{\sigma}_{\epsilon}) = p(y | \hat\beta, \hat\sigma) 
      &= \prod_{i} N(y_{i}| X_{i} \hat{\beta}, \hat{\sigma}_{\epsilon}^{2}) \\
      &=  
        \left(
        \frac{1}{\hat{\sigma}_{\epsilon} \sqrt{2 \pi}}
        \right)^{n}
        \prod_{i} \exp \left(
        - \frac{(y_{i} - x'_{i} \hat{\beta})^{2}}{2 \hat{\sigma}^{2}_{\epsilon}}
        \right) \\
      &=  
        \left(
        \frac{1}{\hat{\sigma}_{\epsilon} \sqrt{2 \pi}}
        \right)^{n}
        \prod_{i} \exp \left(
        - \frac{\hat{\epsilon}_{i}^{2}}{2 \hat{\sigma}^{2}_{\epsilon}}
        \right)
    \end{align*}
  \item For computational stability (the product of probabilities is a small number), the \textbf{log likelihood} is usually used
    \begin{equation*}
      \log L(\hat{\beta}, \hat{\sigma}_{\epsilon}) = - n \log \hat{\sigma}_{\epsilon} - \frac{1}{2} \log 2 \pi - \frac{1}{2 \hat{\sigma}^{2}_{\epsilon}} \sum_{i} \hat{\epsilon}_{i}^{2}
    \end{equation*}
  \end{itemize}
  
\end{frame}

\begin{frame}
  \frametitle{Information Criteria}

  \begin{itemize}
  \item Information criteria include log Likelihod + a penalty for complexity
  \item The two Most common are AIC and BIC:
    \begin{align*}
      AIC &= -2 \log L(\hat{\beta}, \hat{\sigma}_{\epsilon}) + 2 k \\
      BIC &= -2 \log L(\hat{\beta}, \hat{\sigma}_{\epsilon}) + k \log n
    \end{align*}
  \end{itemize}
  \begin{itemize}
  \item Lower is better
  \item Smaller values = better fit
  \item See Fox for justifications
  \item AIC = approx leave one out cross-validation; BIC = a specific k-fold cross-validation
  \end{itemize}

\end{frame}

\subsection{Out-of-Sample and Cross-Validation Method}

\begin{frame}
  \frametitle{Out of Sample Methods}

  \begin{itemize}
  \item Compare models on how well they do on data that was not used to estimate their parameters.
  \item In practice, serves as a good check against spurious findings
  \item Even if our goal is explanation, not prediction, scientific models strive for generality
  \item Usual caveat: best fitting may not be the only criteria for the model
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Out of Sample Goodness of Fit}

  \begin{itemize}
  \item Method
    \begin{enumerate}
    \item Split data into training $(X_{\text{training}}, y_{\text{training}})$, test data, $(X_{\text{test}}, y_{\text{test}})$.
    \item Fit model to training data, $(X_{\text{training}}, y_{\text{training}})$, obtain $\hat{\beta}_{\mathtt{training}}$
    \item Calcuate fitted $\hat{y}_{\text{test}}$ for the test sample $(X_{\text{test}}, y_{\text{test}})$.
    \item Calculate predicted mean squared error of the \textbf{test} data
      \begin{equation*}
        RMSE_{\text{prediction}} = \hat{\sigma}_{\text{test}} = \sqrt{\frac{1}{n_{\text{test}}} \sum_{i \in \text{test}} \hat{\epsilon}_{i}^{2}}
      \end{equation*}
    \end{enumerate}
  \item Usually MSE of test data lower than MSE of training data. In-sample fit statistics are overly optimistic.
  \item Good rule of thumb: 70--75\% training, 30--25\% test
  \item Can use other prediction statistics to evaulate models
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Cross Validation}

  Reuse data for multiple in-sample and out-of-sample tests. More efficient use of data. 
  \begin{itemize}
  \item $k$-fold cross validation
    \begin{enumerate}
    \item Select all but $1/k$th of the data: $(y_{\text{training}}, X_{\text{training}})$
    \item Repeat out of sample tests $k$ times
    \end{enumerate}
  \item Leave-one-out (LOO-CV): $k = n$.
  \item 5-- or 10--fold cross-validation; generally the best in terms of bias / variance tradeoff.
  \item The best model minimizes prediction RMSE
  \item \textbf{Important:} the test and trainining data should be from same ``population''.
    Randomly sampled in cross-section. Need to be careful in panel, blocked, or time-series.
  \end{itemize}

\end{frame}

\section{General Advice on Model Selection}

\begin{frame}
  \frametitle{Fox on Model Selection}
  \framesubtitle{Problems}

  \begin{itemize}
  \item Simultaneous inference
  \item Fallacy of affirming the consequent
  \item Impact of large samples on hypothesis tests
  \item Exaggerated precision
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Fox on Model Selection}
  \framesubtitle{Strategies}

  \begin{itemize}
  \item Alternative model-selection criteria (not stat sig)
  \item Compensating for simulaneous inference
  \item Avoiding model selection: maximally complex and flexible model.
  \item Model averaging: select many models.
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Fox on Model Selection}
  \framesubtitle{General Advice}

  \begin{itemize}
  \item It is problematic to use stat. hypoth. tests for model selection. Simultaneous inference,
    biased results. Complicated models in large $n$, exaggerated prediction. (p. 6008)
  \item Most methods maximize \textit{predication} not interpretation
  \item When purpose is interpretation, simplify based on substantive considerations,
    even if that includes removing small, but stat sig coefficients. (p. 622)
  \item \textbf{validation}: using separate model choice and inference
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Gelman and Hill's Rules for Building a Regression Model for Prediction}

  \begin{itemize}
  \item Include all input variables expected to be important in predicting outcome (substantively)
  \item Not always necessary to include these separately, e.g. indices
  \item For inputs with large effects, consider including interactions
  \item Whether to exclude a varaible from prediction based on significance
    \begin{itemize}
    \item Not stat sig, expected sign: keep. Will not help much, but will not hurt predictions.
    \item Not stat sig, not expected sign: consider removing
    \item Stat sig, not expected sign: \textbf{Think hard} Are there lurking variables?
    \item Stat sig, expected sign: keep
    \end{itemize}
  \item Think hard before the model; but adjust to new information
  \item Gelman and Hill use \textit{predictaion} differently than Fox.
  \end{itemize}

  Gelman and Hill, p. 69
\end{frame}

\section{References}

\begin{frame}
  \frametitle{References}
  
  \begin{itemize}
  \item John Fox, \textit{Applied Regression Analysis and Generalized Linear Models}, Ch. 22, ``Model Selection, Averaging, and Validation''.
  \item Christopher Adolph (Spring 2014) ``Linear Regression: Specification and Fitting'' [Lecture slides]. \url|http://faculty.washington.edu/cadolph/503/topic5.pw.pdf|.
  \end{itemize}
  
\end{frame}

\end{document}
