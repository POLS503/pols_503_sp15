% Copyright (C) 2015 Jeffrey B. Arnold
% License CC BY-NC-SA 4.0 http://creativecommons.org/licenses/by-nc-sa/4.0/

<<init,echo=FALSE,results='hide',warning=FALSE,message=FALSE>>=
source("init.R")
@
<<header>>=
suppressPackageStartupMessages({
  library("car")
  library("gapminder")
})
@

\input{\jobname-options}
\ifdefined\ishandout%
  \documentclass[handout]{beamer}
\else
  \documentclass[]{beamer}
\fi

%%%INSERTHEADERHERE

\input{includes.tex}

\usepackage{tikz}

\newcommand{\thetitle}{Binary Dependent Variables}
\date{May 26, 2015}
\title{\thetitle{}}
\hypersetup{
  pdftitle={\thetitle{}},
  pdfkeywords={statistics}
}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

\DeclareMathOperator{\logit}{logit}
\begin{document}

\begin{frame}
  \maketitle{}
\end{frame}

\begin{frame}
\frametitle{Overview}
\tableofcontents
\end{frame}

\section{Linear Probability Model}


\begin{frame}
\frametitle{Example of Linear Probability Model}
\framesubtitle{Vote for Bush in U.S. Presidential Election 1992}

<<>>=
nes <- read.dta("../data/nes5200_processed_voters_realideo.dta")
nes_sample <- nes %>%
  filter(year == 1992) %>%
  filter(! is.na(presvote) & ! is.na(income)) %>%
  sample_n(1000) %>%
  mutate(voterep = as.integer(presvote == "2. republican"),
         income = as.integer(income))
ggplot(nes_sample, aes(x = income, y = voterep)) +
  geom_point(position = position_jitter(h = 0.1), colour = "gray") +
  geom_smooth(method = "lm") +
  ylab("Voted for Bush (1992)") +
  xlab("Income Category (low to high)") +
  theme_local()
@

\end{frame}

\begin{frame}
  \frametitle{Residuals in LPM}
  \framesubtitle{Vote for Bush in U.S. Presidential Election 1992}

<<>>=
mod <- augment(lm(voterep ~ income, data = nes_sample))
ggplot(mod, aes(x = income, y = .resid)) +
  geom_point(position = position_jitter(h = 0.1)) +
  geom_hline(yintercept = 0, colour = "gray") +
  ylab(expression(hat(epsilon[i]))) +
  theme_local()

@

\end{frame}


\begin{frame}
  \frametitle{Residuals Squared in LPM}
  \framesubtitle{Vote for Bush in U.S. Presidential Election 1992}

<<>>=
mod <- augment(lm(voterep ~ income, data = nes_sample))
ggplot(mod, aes(x = income, y = abs(.resid)^2)) +
  geom_point(position = position_jitter(h = 0.1)) +
  ylab(expression(abs(hat(epsilon)[i]^2))) +
  theme_local()

@

\end{frame}



\begin{frame}
\frametitle{Example of Linear Probability Model}
\framesubtitle{Vote Intention in Chilean Plebiscite in 1973}

<<warning = FALSE>>=
data("Chile", package = "car")
Chile <- mutate(Chile, vote_yes = as.integer((vote == "Y")))
ggplot(Chile,
       aes(x = statusquo, y = vote_yes)) +
  geom_point(position = position_jitter(h = 0.1), colour = "gray") +
  geom_smooth(method = "lm") +
  ylab("Voting Intention") +
  xlab("Support for Status Quo") +
  theme_local()
@


\end{frame}


\begin{frame}
  \frametitle{Residuals in LPM}
  \framesubtitle{Vote Intention in Chilean Plebiscite in 1973}

<<warning = FALSE>>=
mod2 <- lm(vote_yes ~ statusquo, data = Chile)
ggplot(augment(mod2),
       aes(x = statusquo, y = .resid)) +
  geom_point(position = position_jitter(h = 0.1)) +
  geom_hline(yintercept = 0, colour = "gray") +
  ylab(expression(hat(epsilon)[i])) +
  xlab("Support for Status Quo") +
  theme_local()
@

\end{frame}


\begin{frame}
  \frametitle{Residuals Squared in LPM}
  \framesubtitle{Vote Intention in Chilean Plebiscite in 1973}

<<warning = FALSE>>=
mod2 <- lm(vote_yes ~ statusquo, data = Chile)
ggplot(augment(mod2),
       aes(x = statusquo, y = abs(.resid)^2)) +
  geom_point(position = position_jitter(h = 0.1)) +
  geom_hline(yintercept = 0, colour = "gray") +
  ylab(expression(abs(hat(epsilon)[i])^2)) +
  xlab("Support for Status Quo") +
  theme_local()
@

\end{frame}


\begin{frame}
  \frametitle{Linear Probability Model}

  OLS with a binary dependent variable.
  When $Y_{i} \in \{0, 1\}$:
  \begin{equation*}
    Y_{i} = \alpha + \beta X_{i} + \epsilon_{i}
  \end{equation*}

  The expected value is a probability
  \begin{equation*}
    E(Y_{i} | X_{i}) = \Pr(Y_{i} = 1 | X_{i}) = \alpha + \beta X_{i}
  \end{equation*}

\end{frame}

\begin{frame}
  \frametitle{Problems with the LPM}

  \begin{itemize}
  \item Errors are not normally distributed
    \begin{align*}
      \epsilon_{i} = 1 - E(Y_{i} | X_{i}) &= 1 - (\alpha - \beta X_{i}) = 1 - \pi_{i} \\
      \epsilon_{i} = 0 - E(Y_{i} | X_{i}) &= 0 - (\alpha - \beta X_{i}) = - \pi_{i}
    \end{align*}
  \item Errors have non-constant variance (heteroskedasticity)
    \begin{equation*}
      V(\epsilon_{i}) = \pi (1 - \pi_{i})
    \end{equation*}
  \item $E(Y_{i} | X_{i}) = \alpha + \beta X_{i}$ can extend beyond (0, 1)
  \item Improper specification leads to bias; heteroskedasticity and errors leads to incorrect standard errors.
  \end{itemize}

\end{frame}



\section{Logit Models}


\begin{frame}
  \frametitle{Logit and Logistic Function}

  \begin{center}
\begin{tikzpicture}[>=latex,scale=1.2]

  \node (R) at (0, 0) {$\mathbb{R} = (-\infty, \infty)$} ;
  \node (p) at (5, 0) {$(0, 1)$};

  \draw[->] (p) to [bend right=45]
  node[midway,above] {$\logit(x) = \log \left( \frac{x}{1 - x} \right)$}
  (R);
  \draw[->] (R) to [bend right=45]
  node[midway,below] {$\logit^{-1}(x) = \frac{e^{x}}{e^{x} + 1} = \frac{1}{1 + e^{-x}}$}
  (p);

\end{tikzpicture}
  \end{center}

\end{frame}


\begin{frame}
  \frametitle{Logit and Logistic Function}

  \begin{block}{Logit Function}
    Function $(0, 1) \to (\infty, -\infty)$
    \begin{equation*}
    \logit(p) = \log \left(\frac{p}{1 - p}\right) = \log(p) - \log(1 - p)
    \end{equation*}
    Interpreted as the log of the odds ratio ($p / (1 - p)$).
  \end{block}

  \begin{block}{Logistic or Inverse Logit Function}
    Function $(\infty, -\infty) \to (0, 1)$
    \begin{equation*}
      \logit^{-1}(x) =
      \frac{1}{1 + \exp(-x)} = \frac{\exp(x)}{\exp(x) + 1}
    \end{equation*}
  \end{block}

  Logistic and logit functions are inverses of each other
  \begin{equation*}
    \logit^{-1}(\logit(x)) = x
  \end{equation*}

\end{frame}


\begin{frame}[fragile]
  \frametitle{Logit Function}
<<>>=
ggplot(data_frame(x = seq(0.05, 0.95, by = 0.01),
                  y = log(x) - log(1 - x)),
       aes(x = x, y = y)) +
  geom_line() +
  xlab("p") +
  ylab("logit^-1 (p)") +
  theme_local()
@

$\logit(p) = \log \left(\frac{p}{1 - p} \right)$

\end{frame}

\begin{frame}[fragile]
\frametitle{Inverse Logit (Logistic) Function}
<<>>=
ggplot(data_frame(x = seq(-5, 5, by = 0.1),
                  y = 1 / (1 + exp(- x))),
       aes(x = x, y = y)) +
  geom_line() +
  xlab("x") +
  ylab("1 / (1 + exp(- x))") +
  theme_local()
@

$\logit^{-1}(x) = \frac{1}{1 + e^{-x}} = \frac{e^{x}}{e^{x} + 1}$

\end{frame}


\begin{frame}
  \frametitle{Logit Objective Function}

  OLS minimizes squared errors
  \begin{equation*}
    \hat\beta = \argmin_{b}  \sum_{i} {(y_{i} - X_{i} b)}^{2}
  \end{equation*}

  Logit minimizes a \textbf{different} function
  \begin{align*}
    \hat{\beta} &= \argmin_{b} \sum_{i} \left(y_{i} \log P_{i} +  (1 - y_{i}) \log (1 - P_{i}) \right) \\
    P_{i} &= \logit^{-1}(X_{i} b) = \frac{1}{1 + \exp(- X_{i} b)}
  \end{align*}
  
  Logit needs to be estimated by an interative maximization method

\end{frame}

\begin{frame}
  \frametitle{Logit Model}
  
  In logit, $\Pr(Y_{i} = 1)$ not $Y_{i}$ is directly a function of $X_{i} \beta$

  \begin{itemize}
  \item Probabilty of $Y_{i} = 1$:
    \begin{align*}
    \Pr(Y_{i} = 1) &= f(X_{i} \beta) \\
    &= \frac{1}{1 + \exp(-(X_{i} \beta))} \\
    &= \logit^{-1}(X_{i} \beta)
    \end{align*}
  \item Alternative interpretation, log odds ratio ($\log(p / (1 - p))$):
    \begin{align*}
    \Pr(Y_{i} = 1) &= \pi_{i} \\
    \logit(\pi_{i}) &= \alpha + X_{i} \beta
    \end{align*}
  \end{itemize}

\end{frame}


\begin{frame}[fragile]

<<echo=TRUE, results = "markup", size = "tiny">>=
summary(glm(voterep ~ income, data = nes_sample,
            family = binomial(link = "logit")))
@

\end{frame}

\begin{frame}
\frametitle{Example of Linear Probability Model}
\framesubtitle{Vote for Bush in U.S. Presidential Election 1992}

<<>>=
ggplot(nes_sample, aes(x = income, y = voterep)) +
  geom_point(position = position_jitter(h = 0.1), colour = "gray") +
  geom_smooth(method = "lm", se = FALSE, colour = "red") +
  geom_smooth(method = "glm", method.args = list(family = binomial(link = "logit"))) +
  ylab("Voted for Bush (1992)") +
  xlab("Income Category (low to high)") +
  theme_local()
@

\end{frame}

\begin{frame}[fragile]

<<echo = TRUE, size = "tiny">>=
summary(glm(vote_yes ~ statusquo, data = Chile,
            family = binomial(link = "logit")))
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Example of Linear Probability Model}
\framesubtitle{Vote Intention in Chilean Plebiscite in 1973}

<<warning = FALSE>>=
ggplot(Chile,
       aes(x = statusquo, y = vote_yes)) +
  geom_point(position = position_jitter(h = 0.1), colour = "gray") +
  geom_smooth(method = "lm", se = FALSE, colour = "red") +
  geom_smooth(method = "glm", method.args = list(family = binomial(link = "logit"))) +
  ylab("Voting Intention") +
  xlab("Support for Status Quo") +
  theme_local()
@


\end{frame}


\begin{frame}
  \frametitle{Logit Coefficients are Less Transparent}

  \begin{block}{Linear Regression Coeficients}
    
  \begin{equation*}
    \frac{\partial Y}{\partial X_{j}} = \frac{\partial}{\partial X_{j}} (\alpha + \beta_{1} X_{1} + \dots \beta_{k} X_{k}) = \beta_{j}
  \end{equation*}
  Coefficient equals the marginal effect of $x$
\end{block}

  \begin{block}{Logistic Regression Coeficients}
  \begin{equation*}
    \frac{\partial \logit(Y)}{\partial X_{j}} = \frac{\partial}{\partial X_{j}} 
    \left(
       \frac{1}{1 + \exp(\alpha + \beta X_{i})}
    \right) = \Pr(Y = 1 | X_{i}) \Pr(Y = 0 | X_{i}) \beta_j
  \end{equation*}
  or
  \begin{equation*}
    \frac{\partial \logit(Y)}{\partial X_{j}} = \frac{\partial}{\partial X_{j}} X_{i} \beta_{j} = \beta_j
  \end{equation*}
  Coefficient does not equal the marginal effect of $x_{j}$
  \end{block}

\end{frame}



\section{LPM vs. Logit}

\begin{frame}
  \frametitle{The LPM Strikes Back}

  \begin{itemize}
  \item LPM has renewed popular among econometricians, causal inference folks -
  \item See the debate \href{http://www.mostlyharmlesseconometrics.com/2012/07/probit-better-than-lpm/}{here}
  \item OLS is still Min MSE linear approx of Conditional Expectation Function
  \item If the functional form is wrong ; but so it logit / probit.
    And the functional form is \textbf{always} wrong; 
  \item OLS coefficients are a good estimate of the average marginal effects even if not good for the marginal effects at a given $x$.
  \item OLS coefficients are directly interpretable
  \item Angrist and Pischke recommend LPM with heteroskedasticity consistent errors
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Average Marginal Effects}

  \begin{itemize}
  \item The \textbf{average marginal effect} summarizes the marginal effect $\frac{\partial y}{\partial x_{j}}$ averaging over the sample of $x$.
    \begin{equation*}
      \text{Avg. Marginal Effect of $x_{j}$} = \frac{1}{n} \sum_{i} \left. \frac{\partial Y}{\partial x_j} \right|_{X_i}
    \end{equation*}
  \item In OLS, the marginal effect of $x_{j}$ (assuming no interactions, polynomials, etc.) is simply the coefficient
    \begin{equation*}
       \left. \frac{\partial y}{\partial x_j} \right|_{x_i} = \frac{1}{n} \sum_{i} \hat{\beta}_{j} = \hat{\beta}_{j}
    \end{equation*}
  \item In Logit, the average
    \begin{equation*}
       \left. \frac{\partial y}{\partial x_j} \right|_{x_i} = \frac{1}{n} \sum_{i} \Pr(y_{i} = 1 | \hat{\beta}, x_{i})\Pr(y_{i} = 0 | \hat{\beta}, x_{i}) \hat{\beta}_{j} 
    \end{equation*}
  \end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Comparing Average Marginal Effects of Logit and LPM}
\framesubtitle{1992 U.S. Election Example}


<<echo = TRUE>>=
mod <- glm(voterep ~ income, data = nes_sample,
           family = binomial(link = "logit"))
mod_aug <- augment(mod, type.predict = "response")
mean(mod_aug$.fitted * (1 - mod_aug$.fitted) * coef(mod)[2])

lm(voterep ~ income, data = nes_sample)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Comparing Average Marginal Effects of Logit and LPM}
\framesubtitle{Chile Plebiscite Example}

<<echo = TRUE>>=
mod <- glm(vote_yes ~ statusquo, data = Chile,
           family = binomial(link = "logit"))
mod_aug <- augment(mod, type.predict = "response")
mean(mod_aug$.fitted * (1 - mod_aug$.fitted) * coef(mod)[2])

lm(vote_yes ~ statusquo, data = Chile)
@
\end{frame}



\section{References}


\begin{frame}
  \frametitle{References}

  \begin{itemize}
  \item Fox, Ch. 14
  \item Gelman and Hill, Ch 5. This should have most material you need.
  \item Chile Plebicite example: Fox, Ch. 14. Data from \textbf{arm} package dataset \texttt{Chile}.
  \item Bush vote in 1992 example: Gelman and Hill, Ch 5. Data from
    \url{http://www.stat.columbia.edu/~gelman/arm/examples/ARM_Data.zip} as \texttt{ARM\_Data/nes/nes5200\_processed\_voters\_realideo.dta}.
  \end{itemize}
\end{frame}

\end{document}
