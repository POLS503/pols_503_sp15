% Copyright (C) 2015 Jeffrey B. Arnold
% License CC BY-NC-SA 4.0 http://creativecommons.org/licenses/by-nc-sa/4.0/
<<init,echo=FALSE,results='hide',warning=FALSE,message=FALSE>>=
LECTURE_NUM <- "specification"
source("init.R")
@
<<header>>=
suppressPackageStartupMessages({
library("gapminder")
})
@

\input{\jobname-options}
\ifdefined\ishandout%
  \documentclass[handout]{beamer}
\else
  \documentclass[]{beamer}
\fi

% HEADER HERE

\input{includes.tex}


\newcommand{\thetitle}{Transformations}
\date{May 5, 2015}
\title{\thetitle{}}
\hypersetup{
  pdftitle={\thetitle{}},
  pdfkeywords={statistics}
}

\begin{document}

\begin{frame}
  \maketitle{}
\end{frame}

\begin{frame}
\frametitle{Overview}
\tableofcontents
\end{frame}


\begin{frame}
  \frametitle{Residuals and Misspecification}
  \framesubtitle{Example}

<<>>=
data("gapminder")
ggplot(data = filter(gapminder, year == 2007), aes(x = gdpPercap, y = lifeExp)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_local()
@

\end{frame}

\begin{frame}
  \frametitle{Residuals and Misspecification}
  \framesubtitle{Example}

<<>>=
data("gapminder")
ggplot(data = augment(lm(lifeExp ~ gdpPercap, data = filter(gapminder, year == 2007))),
       aes(x = gdpPercap, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  theme_local()
@

\end{frame}

\begin{frame}
  \frametitle{Residuals and Misspecification}
  \framesubtitle{Example}

<<>>=
data("gapminder")
ggplot(data = filter(gapminder, year == 2007), aes(x = log(gdpPercap), y = lifeExp)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_local()
@

\end{frame}

\begin{frame}
  \frametitle{Residuals and Misspecification}
  \framesubtitle{Example}

<<>>=
data("gapminder")
ggplot(data = augment(lm(lifeExp ~ log(gdpPercap), data = filter(gapminder, year == 2007))),
       aes(x = log.gdpPercap., y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  theme_local()
@

\end{frame}


\section{Logarithms and Power Transformations}


\begin{frame}
  \frametitle{Interpreting Logarithms}

  How would you interpret the following?

  \begin{itemize}
  \item $\text{GDP per cap}_{i} = \alpha + \beta \log \text{(school)}_{i}$
  \item $\log \text{GDP per cap}_{i} = \alpha + \beta \text{(school)}_{i}$
  \item $\log \text{GDP per cap}_{i} = \alpha + \beta \log \text{(school)}_{i}$
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Linearizing Functions}

  Can you linearize these with logarithms?

  \begin{block}{Exponential}
    \begin{equation*}
      y_{i} = e^{\beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2} \epsilon_{i}}
    \end{equation*}
  \end{block}

  \begin{block}{Gravity Equation}
    \begin{equation*}
      \text{trade}_{ij} = \frac{\alpha \text{GDP}_i^{\beta_1} \text{GDP}_j^{\beta_2}}{\delta d_{ij}^{\gamma}}
    \end{equation*}
  \end{block}

  \begin{block}{Cobb-Douglas}
    \begin{equation*}
      y = \alpha (x^{\delta \gamma} x^{(1 - \delta)})^{\gamma}
    \end{equation*}
  \end{block}

  \begin{block}{CES Production Function}
    \begin{equation*}
      y = \alpha (\delta x^\rho + (1 - \delta) x^\rho )^{\gamma / \rho}
    \end{equation*}
  \end{block}

\end{frame}


\begin{frame}
  \frametitle{Interpretating Logarithms}

  \begin{block}{Why use natural log for regression}
  \begin{itemize}
  \item Note: $\log(1 + r) \approx r$ when $r$ small
  \item $$\log(x) - \log(x (1 + r)) = \log(1 + r) \approx r = \% \Delta x / 100$$
  \item Only holds for natural logarithm
  \end{itemize}
  \end{block}

  \begin{block}{Converting between bases}
  To convert $\log_e$ to $\log_{10}$
  $$
  \log_{10}(x) = \frac{\log_e(x)}{\log_e(10)}
  $$
  \end{block}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Box-Cox Family of Transformations}


<<>>=
.dat <- expand.grid(lambda = c(-2, -1, -0.5, 0, 0.5, 1, 2, 3)) %>%
  group_by(lambda) %>%
  do(data_frame(x = seq(0.01, 4, by = 0.01),
                y = car::bcPower(x, .$lambda)))

ggplot(.dat, aes(x = x, y = y, colour = factor(lambda))) + geom_line() +
  theme_local() +
  scale_colour_discrete("lambda") +
  scale_y_continuous(limits = c(-5, 5))

@

Plot for $\lambda = 0.25, 0.5, 0, 2, 4, 8$  for $x = (0, 4]$

\end{frame}

\begin{frame}
  \frametitle{Box-Cox Family of Transforms}

  \begin{equation*}
    \begin{cases}
      f(x, \lambda) = \frac{x^{\lambda} - 1}{\lambda} & \text{if $\lambda \neq 0$}\\
      f(x, \lambda) = \log{x} & \text{if $\lambda = 0$}
    \end{cases}
  \end{equation*}

  \begin{itemize}
  \item Requires $x > 0$. If negative, use $x + c$ (some problems), or Yeo-Johnson
  \item Can solve for $\lambda$ to transform $x$ as close to wrt. Normal skew.
  \item \textbf{car} function: \verb|powerTransform|, \verb|bcTransform|.
  \item In regression: If know $\lambda$ can transform $y$ or $x$
  \end{itemize}

\end{frame}

\section{Linear Transformations of Regressions}

\begin{frame}
  \frametitle{Linear Transformations of Regression}

  \begin{equation*}
    (y_{i} + a) / b = \alpha + \beta (x_{i} + d) / e + \epsilon_{i}
  \end{equation*}

  \begin{equation*}
    (y_{i} + \bar{y}) / s_{y} = \alpha + \beta (x_{i} + \bar{x}) / s_{x} + \epsilon_{i}
  \end{equation*}

\end{frame}

\begin{frame}
  \frametitle{Standardized Coefficients / Regressors}

  \begin{equation*}
    y = \alpha + \beta{0} + \beta_1 \frac{x_i - \bar{x}}{\sd{(x)}} + \epsilon_i
  \end{equation*}

  \begin{itemize}
  \item Can be useful for default interpretation (controversial)
  \item Bad for skewed variables, binary variables? But about same as comparing $X + \sd X$ post-estimation.
  \item Transform regressors, not functions of regressors.
  \item Gelman: Continuous: divide by $2 \sd$; Binary: center at mean.
  \item No need for them for default interpretation. With computational power, simulations better.
  \item Very important to standardize $X$ in machine learning applications, or anywhere with complicated optimization problems.
  \end{itemize}


\end{frame}

\end{document}
