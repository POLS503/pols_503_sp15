---
title: "POLS 503: Homework 3"
author: "Jeffrey B. Arnold, Christopher Adolph"
date: "April 27, 2015"
output:
  html_document:
    toc: true
---

The purpose of this homework is to provide a guided, hands-on tour through the properties
of the least squares estimator, especially under common violations of the GaussMarkov
assumptions. 
We will work through a series of programs which use simulated data –- i.e., data created with known properties –- to investigate how these violations affect the accuracy and precision of least squares estimates of slope parameters. 
Using repeated study of simulated datasets to explore the properties of statistical models is
called Monte Carlo experimentation.
Although you will not have to write much R code, you will need to read through
the provided programs carefully to understand what is happening.

For this assignment, I ask that you start with this code and edit it as needed.
Do not 

```{r setup, echo = TRUE}
# This assignment will have some non-trivial computation.
# This will set the options so that any computations that do not change are not
# rerun.
# See http://rmarkdown.rstudio.com/authoring_rcodechunks.html#caching
knitr::opts_chunk$set(cache = TRUE)
```


This will use the standard [Hadleyverse](http://blog.revolutionanalytics.com/2015/03/hadleyverse.html) packages that we've been using in this course (**ggplot2**, **dplyr**, **tidyr**, **broom**).
Additionally we will use **MASS**, an R package associated with the intro stats and R book *Modern Applied Statistics with S*, which has a function to sample from the multivariate normal distribution.
```{r}
library("dplyr")
library("broom")
library("tidyr")
library("ggplot2")
library("MASS")
```


Calculate a covariance matrix, $\Sigma$ from a standard deviation $s$ and a correlation matrix $P$
$$
\Sigma = diag(s) P diag(s)
$$
This is usually more intuitive than the covariance itself.

## Method 1

We can use **dplyr** function do, but instead of an expression, use a function.
This is an alternative method for the simulations using a function.

n

:  Number of simulations

b

: Coefficients

X

: Model matrix

sigma

: Standard deviation of the

```{r}
sim_linear_normal <- function(sims, obs, b, X, sigma) {
  # Create a list to stor the results
  simulations <- list()
  # Create a progress bar
  p <- progress_estimated(sims, min_time = 2)
  # Loop over the simulation runs
  for (j in 1:sims) {
    # The conditional expectation of y given X
    # i.e. y-hat, the fitted values, regression line
    mu <- X %*% b
    
    # Simulate y from the conditional expection + error
    # y = mu + e
    # mu = X * b
    # e ~ N(0, sigma)
    y <- mu + rnorm(n, mean = 0, sd = sigma)
    
    # Run a regression of the simulated y on the simulated X
    # Note that neither y nor X is in a data frame
    # also X is a matrix
    # R is cool with that
    mod <- lm(y ~ X)
    
    # Create a data.frame with the coefficients, se using the broom function tidy
    mod_df <- tidy(mod) %>%
      # Add a column indicating the simulation number
      mutate(.sim = j)
      
    # Save these results as the next element in the storage list
    simulations[[j]] <- mod_df
    
    # Update the progress bar
    p$tick()$print()
  }
  # Combine the list of data frames into a single data frame
  simulations <- bind_rows(simulations)
}

```



```{r}
sdcor2cov <- function(s, r = diag(length(s))) {
  s <- diag(s, nrow = length(s), ncol = length(s))
  s %*% r %*% s
}
```

```{r}
# Number of simulated datasets to generate
sims <- 1000

# Number of observations in each dataset
n <- 100

# True standard deviation of the regression errors
sigma <- sqrt(2)

# True parameters of the model
b <- rep(1, 4)

# True means of the covariates
mu_x <- rep(0, 3)

# Standard deviations of the covariates
s_x <- rep(1, 3)

# correlation of the covariates
cor_x <- diag(3)
# Create a covariance matrix
sigma_x <- sdcor2cov(s_x, cor_x)
```

```{r}
sim_linear_normal_random_x(1000, 100, mu_x, sigma_x, sigma)
```


## Post-processing

Whever method we've used, once we generate the simulations we can summarize
them in various ways.

The output is a data frame with a row for each iteration, coefficient value.

```{r sim_summary}
sim_summary <- simulations %>%
  group_by(term) %>%
  summarize(estimate_mean = mean(estimate),
            estimate_sd = sd(estimate),
            se_mean = sqrt(mean(std.error) ^ 2)) %>%
  mutate(t_stat_mean = mean_estimate / mean_std_error)

make_term_names <- function(n) {
  c("(Intercept)", paste0("X", 1:(length(b) - 1)))
}

true_parameters <- 
  data.frame(estimate = b,
             term = make_term_names(length(b)))

```


```{r}
ggplot() +
  geom_violin(data = simulations, mapping = aes(x = term, y = estimate)) + 
  geom_point(data = true_parameters, 
             mapping = aes(x = term, y = estimate), colour = "red")

ggplot() +
  geom_density(data = simulations, mapping = aes(x = estimate)) +
  geom_vline(data = true_parameters,
             mapping = aes(xintercept = estimate), colour = "red") +
  facet_wrap( ~ term)


ggplot() +
  geom_boxplot(data = simulations, mapping = aes(x = term, y = estimate)) + 
  geom_point(data = true_parameters, 
             mapping = aes(x = term, y = estimate), colour = "red")

```

```{r}
find_sigma <- function(x, b, signal = 1) {
  n <- nrow(x)
  k <- ncol(x)
  yhat <- x %*% b
  ybar <- mean(yhat)
  TSS <- sum((yhat - ybar) ^ 2)
  TSS / signal / n
}
```

