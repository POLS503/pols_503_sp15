---
title: "POLS 503: Homework 3"
author: "Jeffrey B. Arnold, Christopher Adolph"
date: "April 27, 2015"
output:
  html_document:
    toc: true
---

The purpose of this homework is to provide a guided, hands-on tour through the properties
of the least squares estimator, especially under common violations of the GaussMarkov
assumptions. 
We will work through a series of programs which use simulated data –- i.e., data created with known properties –- to investigate how these violations affect the accuracy and precision of least squares estimates of slope parameters. 
Using repeated study of simulated datasets to explore the properties of statistical models is
called Monte Carlo experimentation.
Although you will not have to write much R code, you will need to read through
the provided programs carefully to understand what is happening.

For this assignment, I ask that you start with this code and edit it as needed.
Do not 

```{r setup, include=FALSE, results = 'hide', echo = FALSE}
# This assignment will have some non-trivial computation.
# This will set the options so that any computations that do not change are not
# rerun.
# See http://rmarkdown.rstudio.com/authoring_rcodechunks.html#caching
knitr::opts_chunk$set(cache = TRUE)
```


Calculate a covariance matrix, $\Sigma$ from a standard deviation $s$ and a correlation matrix $P$
$$
\Sigma = diag(s) P diag(s)
$$
This is usually more intuitive than the covariance itself.



```{r}
library("MASS")

# Number of simulated datasets to generate
sims <- 1000

# Number of observations in each dataset
n <- 100

# True standard deviation of the regression errors
sigma <- sqrt(2)

# True parameters of the model
b <- rep(1, 4)

# True means of the covariates
mu_x <- c(0, 0, 0) + 1

sdcor2cov <- function(s, r = diag(length(s))) {
  s <- diag(s, nrow = length(s), ncol = length(s))
  s %*% r %*% s
}

# Standard deviations of the covariates
s_x <- 1:3
# correlation of the covariates
cor_x <- diag(3)
# Create a covariance matrix
sigma_x <- sdcor2cov(s_x, cor_x)
```

## Method 1: for loop

```{r}
# Create a list to stor the results
simulations <- list()
# Loop over the simulation runs
for (j in 1:sims) {

  # Draw the simulated covariates from their true
  # multivariate Normal distribution
  X <- cbind(1, mvrnorm(n, mu_x, sigma_x))

  # Create the simulated y by
  # adding together the systematic and stochastic
  # components, according to the true model
  mu <- X %*% b
  y <- mu + rnorm(n) * sigma

  # Run a regression of the simulated y on the simulated X
  res <- lm(y ~ X)

  # Save these results as the next element in the storage list
  simulations[[j]] <- mutate(tidy(res), .sim = j)                
}
```
This produces a list of data frames. We'd like them as one big data frame so 
we can use the R function `bind_rows`.
```{r}
simulations <- bind_rows(simulations)
```

## Method 2: do with an expression

Read about the `do` function in **dplyr**. Not only can you run arbitrary

```{r}
simulations <- 
  data_frame(.sim = seq_len(sims)) %>%
  rowwise() %>% 
  do({
    # Draw the simulated covariates from their true
    # multivariate Normal distribution
    X <- mvrnorm(n, mu_x, sigma_x)

    # Create the simulated y by
    # adding together the systematic and stochastic
    # components, according to the true model
    mu <- cbind(1, X) %*% b
    y <- mu + rnorm(n) * sigma

    # Run a regression of the simulated y on the simulated X
    res <- lm(y ~ X)

    # Save these results as the next element in the storage list
    mutate(tidy(res), .sim = .$.sim)  
  })
```


## Method 3: dplyr do with a function

We can use **dplyr** function do, but instead of an expression, use a function.
This is an alternative method for the simulations using a function.

n

:  Number of simulations

x_mu

: Mean of the X distribution

x_Sigma

: Covariance matrix of the X distribution

sigma

: Standard deviation of the 

```{r}
sim_lin_normal <- function(n, b, x_mu, x_Sigma, sigma) {
  
  # Draw the simulated covariates from their true
  # multivariate Normal distribution
  X <- rmvnorm(n, x_mu, x_Sigma)
  
  # Create the simulated y by
  # adding together the systematic and stochastic
  # components, according to the true model
  mu <- cbind(rep(1, n), X) %*% b
  y <- mu + rnorm(n) * sigma
  
  # Run regression, and put coefficients, se, t-stat, and p-values in 
  # a data frame
  tidy(lm(y ~ X))
}
```

Generate a simulations dataset with a value for each simulation. This will hold the output of the simulations. For each simulation, numbered in `.sim`, it run the `sim_lin_normal` to randomly draw $y$s and $X$s, run a regression, and return a summary of the regression.
```{r simulations}
simulations <- 
  data_frame(.sim = seq_len(sims)) %>%
  group_by(.sim) %>% 
  do(sim_lin_normal(n, b, mu_x, sigma_x, sigma))
```

## Post-processing

Whever method we've used, once we generate the simulations we can summarize
them in various ways.

The output is a data frame with a row for each iteration, coefficient value.

```{r sim_summary}
sim_summary <- simulations %>%
  group_by(term) %>%
  summarize(estimate_mean = mean(estimate),
            estimate_sd = sd(estimate),
            se_mean = sqrt(mean(std.error) ^ 2)) %>%
  mutate(t_stat_mean = mean_estimate / mean_std_error)

make_term_names <- function(n) {
  c("(Intercept)", paste0("X", 1:(length(b) - 1)))
}

true_parameters <- 
  data.frame(estimate = b,
             term = make_term_names(length(b)))

```

```{r}
ggplot() +
  geom_violin(data = simulations, mapping = aes(x = term, y = estimate)) + 
  geom_point(data = true_parameters, 
             mapping = aes(x = term, y = estimate), colour = "red")

ggplot() +
  geom_density(data = simulations, mapping = aes(x = estimate)) +
  geom_vline(data = true_parameters,
             mapping = aes(xintercept = estimate), colour = "red") +
  facet_wrap( ~ term)


ggplot() +
  geom_boxplot(data = simulations, mapping = aes(x = term, y = estimate)) + 
  geom_point(data = true_parameters, 
             mapping = aes(x = term, y = estimate), colour = "red")

```

```{r}
select(sim_summary, estimate_mean)
```


```{r}
find_sigma <- function(x, b, signal = 1) {
  n <- nrow(x)
  k <- ncol(x)
  yhat <- x %*% b
  ybar <- mean(yhat)
  TSS <- sum((yhat - ybar) ^ 2)
  TSS / signal / n
}
```


```{r}
#' Simulate data from a normal linear model with fixed X
#' 
#' @param n Number of observations
#' @param b coefficients
#' @param x model matrix.
#' @param sigma standard deviation of the error distribution
#' @returns numeric vector of the y values.
sim_linear_normal <- function(b, X, sigma) {
  X <- as.data.frame(X)
  
  # Create the simulated y by
  # adding together the systematic and stochastic
  # components, according to the true model
  n <- nrow(x)
  mu <- cbind(rep(1, n), x) %*% b
  y <- mu + rnorm(n) * sigma
  
  # Run regression, and put coefficients, se, t-stat, and p-values in 
  # a data frame
  tidy(lm(y ~ X))
}

n <- 15
x <- matrix(rnorm(n))
b <- c(0, 1)
sigma <- 0.5
m <- 1000

simulations <- 
  data_frame(.sim = seq_len(m)) %>%
  group_by(.sim) %>% 
  do(sim_linear_normal(b, x, sigma))

filter(simulations, grepl(term, "^X")) %>% 
  {
    ggplot(data = ., aes(x = estimate)) + 
      geom_density() + 
      stat_function(fun = dnorm,
                    args = list(mean = mean(.$estimate), sd = sd(.$estimate)))  
  }

```
