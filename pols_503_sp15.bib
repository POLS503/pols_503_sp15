
@article{simmons_false-positive_2011,
	title = {False-{Positive} {Psychology} {Undisclosed} {Flexibility} in {Data} {Collection} and {Analysis} {Allows} {Presenting} {Anything} as {Significant}},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	url = {http://pss.sagepub.com/content/22/11/1359},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	language = {en},
	number = {11},
	urldate = {2015-03-26TZ},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	pmid = {22006061},
	keywords = {disclosure, methodology, motivated reasoning, publication},
	pages = {1359--1366}
}

@article{jager_empirical_2013,
	title = {Empirical estimates suggest most published medical research is true},
	url = {http://arxiv.org/abs/1301.3718},
	abstract = {The accuracy of published medical research is critical both for scientists, physicians and patients who rely on these results. But the fundamental belief in the medical literature was called into serious question by a paper suggesting most published medical research is false. Here we adapt estimation methods from the genomics community to the problem of estimating the rate of false positives in the medical literature using reported P-values as the data. We then collect P-values from the abstracts of all 77,430 papers published in The Lancet, The Journal of the American Medical Association, The New England Journal of Medicine, The British Medical Journal, and The American Journal of Epidemiology between 2000 and 2010. We estimate that the overall rate of false positives among reported results is 14\% (s.d. 1\%), contrary to previous claims. We also find there is not a significant increase in the estimated rate of reported false positive results over time (0.5\% more FP per year, P = 0.18) or with respect to journal submissions (0.1\% more FP per 100 submissions, P = 0.48). Statistical analysis must allow for false positives in order to make claims on the basis of noisy data. But our analysis suggests that the medical literature remains a reliable record of scientific progress.},
	urldate = {2015-03-26TZ},
	journal = {arXiv:1301.3718 [stat]},
	author = {Jager, Leah R. and Leek, Jeffrey T.},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.3718},
	keywords = {Statistics - Applications}
}

@book{zuur_beginners_2009,
	address = {New York},
	series = {Use {R}},
	title = {A beginner's guide to {R}},
	isbn = {978-0-387-93837-0},
	url = {http://dx.doi.org/10.1007/978-0-387-93837-0},
	publisher = {Springer},
	author = {Zuur, Alain F and Ieno, Elena N. and Meesters, Erik},
	year = {2009}
}

@article{brambor_understanding_2006,
	title = {Understanding interaction models: improving empirical analyses},
	volume = {14},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Understanding {Interaction} {Models}},
	url = {http://pan.oxfordjournals.org/content/14/1/63},
	doi = {10.1093/pan/mpi014},
	abstract = {Multiplicative interaction models are common in the quantitative political science literature. This is so for good reason. Institutional arguments frequently imply that the relationship between political inputs and outcomes varies depending on the institutional context. Models of strategic interaction typically produce conditional hypotheses as well. Although conditional hypotheses are ubiquitous in political science and multiplicative interaction models have been found to capture their intuition quite well, a survey of the top three political science journals from 1998 to 2002 suggests that the execution of these models is often flawed and inferential errors are common. We believe that considerable progress in our understanding of the political world can occur if scholars follow the simple checklist of dos and don'ts for using multiplicative interaction models presented in this article. Only 10\% of the articles in our survey followed the checklist.},
	language = {en},
	number = {1},
	urldate = {2015-03-20TZ},
	journal = {Political Analysis},
	author = {Brambor, Thomas and Clark, William Roberts and Golder, Matt},
	month = dec,
	year = {2006},
	pages = {63--82}
}

@article{kastellec_using_2007,
	title = {Using {Graphs} {Instead} of {Tables} in {Political} {Science}},
	volume = {null},
	issn = {1541-0986},
	url = {http://journals.cambridge.org/article_S1537592707072209},
	doi = {10.1017/S1537592707072209},
	abstract = {When political scientists present empirical results, they are much more likely to use tables than graphs, despite the fact that graphs greatly increases the clarity of presentation and makes it easier for a reader to understand the data being used and to draw clear and correct inferences. Using a sample of leading journals, we document this tendency and suggest reasons why researchers prefer tables. We argue that the extra work required in producing graphs is rewarded by greatly enhanced presentation and communication of empirical results. We illustrate their benefits by turning several published tables into graphs, including tables that present descriptive data and regression results. We show that regression graphs emphasize point estimates and confidence intervals and that they can successfully present the results of regression models. A move away from tables towards graphs would improve the discipline\&apos;s communicative output and make empirical findings more accessible to every type of audience.Jonathan P. Kastellec (jpk2004@columbia.edu) and Eduardo L. Leoni (eleoni@hmdc.harvard.edu) are Doctoral Candidates in Political Science at Columbia University. The authors\&apos; names appear in alphabetical order. They would like to thank Andrew Gelman, Rebecca Weitz-Shapiro, Gary King, David Epstein, Jeff Gill, Piero Stanig, and three anonymous reviewers for helpful comments and suggestions. We also thank Noah Kaplan, David Park, and Travis Ridout for generously making their data publicly available. Eduardo Leoni is grateful for support from the Harvard MIT Data Center, where he was a fellow while working on this project.We have created a web site, http:\&sol;\&sol;tables2graphs.com, that contains complete replication code for all the graphs that appear in this article, as well as additional graphs that we did not present due to space limitations.},
	number = {04},
	urldate = {2015-03-20TZ},
	journal = {Perspectives on Politics},
	author = {Kastellec, Jonathan P. and Leoni, Eduardo L.},
	month = dec,
	year = {2007},
	pages = {755--771}
}

@article{gelman_lets_2002,
	title = {Let's {Practice} {What} {We} {Preach}},
	volume = {56},
	issn = {0003-1305},
	url = {http://dx.doi.org/10.1198/000313002317572790},
	doi = {10.1198/000313002317572790},
	abstract = {Statisticians recommend graphical displays but often use tables to present their own research results. Could graphs do better? We study the question by going through the tables in a recent issue of the Journal of the American Statistical Association. We show how it is possible to improve the presentations using graphs that actually take up less space than the original tables. We find a particularly effective tool to be multiple repeated line plots, with comparisons of interest connected by lines and separate comparisons isolated on different plots.},
	number = {2},
	urldate = {2015-03-20TZ},
	journal = {The American Statistician},
	author = {Gelman, Andrew and Pasarica, Cristian and Dodhia, Rahul},
	month = may,
	year = {2002},
	pages = {121--130}
}

@article{oneal_rule_2005,
	title = {Rule of {Three}, {Let} {It} {Be}? {When} {More} {Really} {Is} {Better}},
	volume = {22},
	issn = {0738-8942, 1549-9219},
	shorttitle = {Rule of {Three}, {Let} {It} {Be}?},
	url = {http://cmp.sagepub.com/content/22/4/293},
	doi = {10.1080/07388940500339209},
	abstract = {Jim Ray and others in this issue question customary procedures for the quantitative analysis of theoretically complex questions in the social sciences. In this article we address Ray's use of research on the Kantian peace to illustrate his points. We discuss his five guidelines for research, indicating how we agree and disagree, and take up five substantive issues he has raised about our research. With new analyses to supplement our previous work, we show that none of his reservations is well founded. We discuss the costs as well as the benefits of rigid insistence on reducing the number of independent variables in a regression equation.},
	language = {en},
	number = {4},
	urldate = {2015-03-20TZ},
	journal = {Conflict Management and Peace Science},
	author = {Oneal, John R. and Russett, Bruce},
	month = sep,
	year = {2005},
	keywords = {causes of war, democratic peace, economic interdependence, liberal peace},
	pages = {293--310}
}

@article{clarke_phantom_2005,
	title = {The {Phantom} {Menace}: {Omitted} {Variable} {Bias} in {Econometric} {Research}},
	volume = {22},
	issn = {0738-8942, 1549-9219},
	shorttitle = {The {Phantom} {Menace}},
	url = {http://cmp.sagepub.com/content/22/4/341},
	doi = {10.1080/07388940500339183},
	abstract = {Quantitative political science is awash in control variables. The justification for these bloated specifications is usually the fear of omitted variable bias. A key underlying assumption is that the danger posed by omitted variable bias can be ameliorated by the inclusion of relevant control variables. Unfortunately, as this article demonstrates, there is nothing in the mathematics of regression analysis that supports this conclusion. The inclusion of additional control variables may increase or decrease the bias, and we cannot know for sure which is the case in any particular situation. A brief discussion of alternative strategies for achieving experimental control follows the main result.},
	language = {en},
	number = {4},
	urldate = {2015-03-20TZ},
	journal = {Conflict Management and Peace Science},
	author = {Clarke, Kevin A.},
	month = sep,
	year = {2005},
	keywords = {control variables, omitted variable bias, research design, specification},
	pages = {341--352}
}

@article{achen_lets_2005,
	title = {Let's {Put} {Garbage}-{Can} {Regressions} and {Garbage}-{Can} {Probits} {Where} {They} {Belong}},
	volume = {22},
	issn = {0738-8942, 1549-9219},
	url = {http://cmp.sagepub.com/content/22/4/327},
	doi = {10.1080/07388940500339167},
	abstract = {Many social scientists believe that dumping long lists of explanatory variables into linear regression, probit, logit, and other statistical equations will successfully “control” for the effects of auxiliary factors. Encouraged by convenient software and ever more powerful computing, researchers also believe that this conventional approach gives the true explanatory variables the best chance to emerge. The present paper argues that these beliefs are false, and that without intensive data analysis, linear regression models are likely to be inaccurate. Instead, a quite different and less mechanical research methodology is needed, one that integrates contemporary powerful statistical methods with deep substantive knowledge and classic data—analytic techniques of creative engagement with the data.},
	language = {en},
	number = {4},
	urldate = {2015-03-20TZ},
	journal = {Conflict Management and Peace Science},
	author = {Achen, Christopher H.},
	month = sep,
	year = {2005},
	keywords = {data analysis, linearity, monotonicity, regression analysis, rule of three},
	pages = {327--339}
}

@book{freedman_statistics_2007,
	address = {New York},
	edition = {4th edition},
	title = {Statistics, 4th {Edition}},
	isbn = {9780393929720},
	abstract = {Renowned for its clear prose and no-nonsense emphasis on core concepts, Statistics covers fundamentals using real examples to illustrate the techniques. The Fourth Edition has been carefully revised and updated to reflect current data.},
	language = {English},
	publisher = {W. W. Norton \& Company},
	author = {Freedman, David and Pisani, Robert and Purves, Roger},
	month = feb,
	year = {2007}
}

@book{moore_mathematics_2013,
	address = {Princeton, NJ},
	edition = {1 edition},
	title = {A {Mathematics} {Course} for {Political} and {Social} {Research}},
	isbn = {9780691159171},
	abstract = {Political science and sociology increasingly rely on mathematical modeling and sophisticated data analysis, and many graduate programs in these fields now require students to take a "math camp" or a semester-long or yearlong course to acquire the necessary skills. Available textbooks are written for mathematics or economics majors, and fail to convey to students of political science and sociology the reasons for learning often-abstract mathematical concepts. A Mathematics Course for Political and Social Research fills this gap, providing both a primer for math novices in the social sciences and a handy reference for seasoned researchers. The book begins with the fundamental building blocks of mathematics and basic algebra, then goes on to cover essential subjects such as calculus in one and more than one variable, including optimization, constrained optimization, and implicit functions; linear algebra, including Markov chains and eigenvectors; and probability. It describes the intermediate steps most other textbooks leave out, features numerous exercises throughout, and grounds all concepts by illustrating their use and importance in political science and sociology.  Uniquely designed and ideal for students and researchers in political science and sociology Uses practical examples from political science and sociology Features "Why Do I Care?" sections that explain why concepts are useful Includes numerous exercises Complete online solutions manual (available only to professors, email david.siegel at duke.edu, subject line "Solution Set") Selected solutions available online to students},
	language = {English},
	publisher = {Princeton University Press},
	author = {Moore, Will H. and Siegel, David A.},
	month = aug,
	year = {2013}
}

@book{gailmard_statistical_2014,
	address = {New York, NY},
	title = {Statistical {Modeling} and {Inference} for {Social} {Science}},
	isbn = {9781107003149},
	abstract = {This book provides an introduction to probability theory, statistical inference, and statistical modeling for social science researchers and Ph.D. students. Focusing on the connection between statistical procedures and social science theory, Sean Gailmard develops core statistical theory as a set of tools to model and assess relationships between variables - the primary aim of social scientists. Gailmard explains how social scientists express and test substantive theoretical arguments in various models. Chapter exercises require application of concepts to actual data and extend students' grasp of core theoretical concepts. Students will complete the book with the ability to read and critique statistical applications in their fields of interest.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Gailmard, Sean},
	month = jun,
	year = {2014}
}

@book{morgan_counterfactuals_2014,
	address = {New York, NY},
	edition = {2 edition},
	title = {Counterfactuals and {Causal} {Inference}: {Methods} and {Principles} for {Social} {Research}},
	isbn = {9781107694163},
	shorttitle = {Counterfactuals and {Causal} {Inference}},
	abstract = {In this second edition of Counterfactuals and Causal Inference, completely revised and expanded, the essential features of the counterfactual approach to observational data analysis are presented with examples from the social, demographic, and health sciences. Alternative estimation techniques are first introduced using both the potential outcome model and causal graphs; after which, conditioning techniques, such as matching and regression, are presented from a potential outcomes perspective. For research scenarios in which important determinants of causal exposure are unobserved, alternative techniques, such as instrumental variable estimators, longitudinal methods, and estimation via causal mechanisms, are then presented. The importance of causal effect heterogeneity is stressed throughout the book, and the need for deep causal explanation via mechanisms is discussed.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Morgan, Stephen L. and Winship, Christopher},
	month = nov,
	year = {2014}
}

@book{gelman_data_2006,
	address = {Cambridge ; New York},
	edition = {1 edition},
	title = {Data {Analysis} {Using} {Regression} and {Multilevel}/{Hierarchical} {Models}},
	isbn = {9780521686891},
	abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout. Author resource page: http://www.stat.columbia.edu/{\textasciitilde}gelman/arm/},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Gelman, Andrew and Hill, Jennifer},
	month = dec,
	year = {2006}
}

@book{fox_applied_2008,
	address = {Los Angeles},
	edition = {2nd edition},
	title = {Applied {Regression} {Analysis} and {Generalized} {Linear} {Models}},
	isbn = {9780761930426},
	abstract = {Combining a modern, data-analytic perspective with a focus on applications in the social sciences, the Second Edition of Applied Regression Analysis and Generalized Linear Models provides in-depth coverage of regression analysis, generalized linear models, and closely related methods. Although the text is largely accessible to readers with a modest background in statistics and mathematics, author John Fox also presents more advanced material throughout the book. Key Updates to the Second Edition:Provides greatly enhanced coverage of generalized linear models, with an emphasis on models for categorical and count dataOffers new chapters on missing data in regression models and on methods of model selectionIncludes expanded treatment of robust regression, time-series regression, nonlinear regression, and nonparametric regressionIncorporates new examples using larger data setsIncludes an extensive Web site at http://www.sagepub.com/fox that presents appendixes, data sets used in the book and for data-analytic exercises, and the data-analytic exercises themselves Intended Audience: This core text will be a valuable resource for graduate students and researchers in the social sciences (particularly sociology, political science, and psychology) and other disciplines that employ linear and related models for data analysis.},
	language = {English},
	publisher = {SAGE Publications, Inc},
	author = {Fox, John},
	month = apr,
	year = {2008}
}

@book{zuur_beginners_2009-1,
	address = {Dordrecht ; New York},
	edition = {2009 edition},
	title = {A {Beginner}'s {Guide} to {R}},
	isbn = {9780387938363},
	abstract = {Based on their extensive experience with teaching R and statistics to applied scientists, the authors provide a beginner's guide to R. To avoid the difficulty of teaching R and statistics at the same time, statistical methods are kept to a minimum. The text covers how to download and install R, import and manage data, elementary plotting, an introduction to functions, advanced plotting, and common beginner mistakes. This book contains everything you need to know to get started with R. “Its biggest advantage is that it aims only to teach R...It organizes R commands very efficiently, with much teaching guidance included. I would describe this book as being handy--it’s the kind of book that you want to keep in your jacket pocket or backpack all the time, ready for use, like a Swiss Army knife.” (Loveday Conquest, University of Washington) “Whilst several books focus on learning statistics in R..., the authors of this book fill a gap in the market by focusing on learning R whilst almost completely avoiding any statistical jargon...The fact that the authors have very extensive experience of teaching R to absolute beginners shines throughout.” (Mark Mainwaring, Lancaster University) “Exactly what is needed...This is great, nice work. I love the ecological/biological examples; they will be an enormous help.” (Andrew J. Tyne, University of Nebraska-Lincoln)},
	language = {English},
	publisher = {Springer},
	author = {Zuur, Alain and Ieno, Elena N. and Meesters, Erik},
	month = jul,
	year = {2009}
}

@book{davidson_econometric_2003,
	address = {New York},
	title = {Econometric {Theory} and {Methods}},
	isbn = {9780195123722},
	abstract = {Econometric Theory and Methods provides a unified treatment of modern econometric theory and practical econometric methods. The geometrical approach to least squares is emphasized, as is the method of moments, which is used to motivate a wide variety of estimators and tests. Simulation methods, including the bootstrap, are introduced early and used extensively.The book deals with a large number of modern topics. In addition to bootstrap and Monte Carlo tests, these include sandwich covariance matrix estimators, artificial regressions, estimating functions and the generalized method of moments, indirect inference, and kernel estimation. Every chapter incorporates numerous exercises, some theoretical, some empirical, and many involving simulation.Econometric Theory and Methods is designed for beginning graduate courses. The book is suitable for both one- and two-term courses at the Masters or Ph.D. level. It can also be used in a final-year undergraduate course for students with sufficient backgrounds in mathematics and statistics.FEATURES·Unified Approach: New concepts are linked to old ones whenever possible, and the notation is consistent both within and across chapters wherever possible.·Geometry of Ordinary Least Squares: Introduced in Chapter 2, this method provides students with valuable intuition and allows them to avoid a substantial amount of tedious algebra later in the text.·Modern Concepts Introduced Early: These include the bootstrap (Chapter 4), sandwich covariance matrices (Chapter 5), and artificial regressions (Chapter 6).·Inclusive Treatment of Mathematics: Mathematical and statistical concepts are introduced as they are needed, rather than isolated in appendices or introductory chapters not linked to the main body of the text.·Advanced Topics: Among these are models for duration and count data, estimating equations, the method of simulated moments, methods for unbalanced panel data, a variety of unit root and cointegration tests, conditional moment tests, nonnested hypothesis tests, kernel density regression, and kernel regression.·Chapter Exercises: Every chapter offers numerous exercises, all of which have been answered by the authors in the Instructor's Manual. Particularly challenging exercises are starred and their solutions are available at the authors' website, providing a way for instructors and interested students to cover advanced material.},
	language = {English},
	publisher = {Oxford University Press},
	author = {Davidson, Russell and MacKinnon, James G.},
	month = oct,
	year = {2003}
}

@book{wooldridge_econometric_2010,
	address = {Cambridge, Mass},
	edition = {second edition edition},
	title = {Econometric {Analysis} of {Cross} {Section} and {Panel} {Data}},
	isbn = {9780262232586},
	abstract = {The second edition of this acclaimed graduate text provides a unified                 treatment of two methods used in contemporary econometric research, cross section                 and data panel methods. By focusing on assumptions that can be given behavioral                 content, the book maintains an appropriate level of rigor while emphasizing                 intuitive thinking. The analysis covers both linear and nonlinear models, including                 models with dynamics and/or individual heterogeneity. In addition to general                 estimation frameworks (particular methods of moments and maximum likelihood),                 specific linear and nonlinear methods are covered in detail, including probit and                 logit models and their multivariate, Tobit models, models for count data, censored                 and missing data schemes, causal (or treatment) effects, and duration                 analysis.Econometric Analysis of Cross Section and Panel Data was the first graduate                 econometrics text to focus on microeconomic data structures, allowing assumptions to                 be separated into population and sampling assumptions. This second edition has been                 substantially updated and revised. Improvements include a broader class of models                 for missing data problems; more detailed treatment of cluster problems, an important                 topic for empirical researchers; expanded discussion of "generalized instrumental                 variables" (GIV) estimation; new coverage (based on the author's own recent                 research) of inverse probability weighting; a more complete framework for estimating                 treatment effects with panel data, and a firmly established link between econometric                 approaches to nonlinear panel data and the "generalized estimating equation"                 literature popular in statistics and other fields. New attention is given to                 explaining when particular econometric methods can be applied; the goal is not only                 to tell readers what does work, but why certain "obvious" procedures do not. The                 numerous included exercises, both theoretical and computer-based, allow the reader                 to extend methods covered in the text and discover new insights.},
	language = {English},
	publisher = {The MIT Press},
	author = {Wooldridge, Jeffrey M.},
	month = oct,
	year = {2010}
}

@book{greene_econometric_2011,
	address = {Boston},
	edition = {7 edition},
	title = {Econometric {Analysis}},
	isbn = {9780131395381},
	abstract = {Econometric Analysis  serves as a bridge between an introduction to the field of econometrics and the professional literature for  social scientists and other professionals in the field of social sciences, focusing on applied econometrics and theoretical background. This book provides a broad survey of the field of econometrics that allows the reader to move from here to practice in one or more specialized areas. At the same time, the reader will gain an appreciation of the common foundation of all the fields presented and use the tools they employ.},
	language = {English},
	publisher = {Prentice Hall},
	author = {Greene, William H.},
	month = feb,
	year = {2011}
}

@book{venables_modern_2003,
	address = {New York},
	edition = {4th edition},
	title = {Modern {Applied} {Statistics} with {S}},
	isbn = {9780387954578},
	abstract = {A guide to using S environments to perform statistical analyses providing both an introduction to the use of S and a course in modern statistical methods. The emphasis is on presenting practical problems and full analyses of real data sets.},
	language = {English},
	publisher = {Springer},
	author = {Venables, W. N. and Ripley, B. D.},
	month = sep,
	year = {2003}
}

@book{gujarati_basic_2008,
	address = {Boston},
	edition = {5 edition},
	title = {Basic {Econometrics}},
	isbn = {9780073375779},
	abstract = {Gujarati and Porter's Basic Econometrics provides an elementary but comprehensive introduction to econometrics without resorting to matrix algebra, calculus, or statistics beyond the elementary level.  With the addition of over 100 new data sets, as well as significantly updated research and examples, the Fifth Edition responds to important developments in the theory and practice of econometrics. Basic Econometrics is widely used by students of all fields as the expanded topics and concrete applications throughout the text apply to a broad range of studies.},
	language = {English},
	publisher = {McGraw-Hill/Irwin},
	author = {Gujarati, Damodar and Porter, Dawn},
	month = oct,
	year = {2008}
}

@book{cowpertwait_introductory_2009,
	address = {Dordrecht ; New York},
	edition = {2009 edition},
	title = {Introductory {Time} {Series} with {R}},
	isbn = {9780387886978},
	abstract = {This book gives the reader a step-by-step introduction to analyzing time series using the open source software R. Each time series model is illustrated through practical applications addressing contemporary issues, and is defined in mathematical notation.},
	language = {English},
	publisher = {Springer},
	author = {Cowpertwait, Paul S. P. and Metcalfe, Andrew V.},
	month = jun,
	year = {2009}
}

@book{angrist_mostly_2009,
	address = {Princeton},
	edition = {1 edition},
	title = {Mostly {Harmless} {Econometrics}: {An} {Empiricist}'s {Companion}},
	isbn = {9780691120355},
	shorttitle = {Mostly {Harmless} {Econometrics}},
	abstract = {The core methods in today's econometric toolkit are linear regression for statistical control, instrumental variables methods for the analysis of natural experiments, and differences-in-differences methods that exploit policy changes. In the modern experimentalist paradigm, these techniques address clear causal questions such as: Do smaller classes increase learning? Should wife batterers be arrested? How much does education raise wages? Mostly Harmless Econometrics shows how the basic tools of applied econometrics allow the data to speak.  In addition to econometric essentials, Mostly Harmless Econometrics covers important new extensions--regression-discontinuity designs and quantile regression--as well as how to get standard errors right. Joshua Angrist and Jörn-Steffen Pischke explain why fancier econometric techniques are typically unnecessary and even dangerous. The applied econometric methods emphasized in this book are easy to use and relevant for many areas of contemporary social science.  An irreverent review of econometric essentials  A focus on tools that applied researchers use most  Chapters on regression-discontinuity designs, quantile regression, and standard errors  Many empirical examples  A clear and concise resource with wide applications},
	language = {English},
	publisher = {Princeton University Press},
	author = {Angrist, Joshua D. and Pischke, Jörn-Steffen},
	month = jan,
	year = {2009}
}

@article{king_how_1986,
	title = {How {Not} to {Lie} {With} {Statistics}: {Avoiding} {Common} {Mistakes} in {Quantitative} {Political} {Science}},
	volume = {30},
	shorttitle = {How {Not} to {Lie} {With} {Statistics}},
	journal = {American Journal of Political Science},
	author = {King, Gary},
	year = {1986},
	pages = {666--687}
}

@article{king_publication_2006,
	title = {Publication, {Publication}},
	volume = {39},
	journal = {PS: Political Science and Politics},
	author = {King, Gary},
	year = {2006},
	pages = {119--125}
}